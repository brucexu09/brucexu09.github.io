@INPROCEEDINGS{KAN-GNN,
  author = {Wang, Zihu and Xu, Boxun and Geng, Hejia and Li, Peng},
  booktitle={Arxiv}, 
  title={Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives}, 
  year={2025},
  preview={KAN-GNN.png},
  selected=True,
  }

@INPROCEEDINGS{3DMoE,
  author = {Xu, Boxun and Hwang, Junyoung and Vanna-iampikul, Pruek and Yin, Yuxuan and Lim, Sung Kyu and Li, Peng},
  booktitle={ACM/IEEE International Conference on Computer-Aided Design (ICCAD)}, 
  title={3D Acceleration for Mixture-of-Experts and Multi-Head Attention Spiking Transformers with Dynamic Head Pruning}, 
  year={2025},
  preview={3DMoE.png},
  selected=True,
  }

@INPROCEEDINGS{TransferLearningVmin,
  author = {Yin, Yuxuan and Chen, Rebecca and Xu, Boxun and He, Chen and Li, Peng},
  booktitle={ACM/IEEE International Test Conference (ITC)}, 
  title={Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing}, 
  year={2025},
  preview={ITC.png},
  selected=True,
  }

@INPROCEEDINGS{SpikeHQ,
  author = {Xu, Boxun and Song, Yufei and Li, Peng},
  booktitle={IEEE International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 
  title={Trimming Down Large Spiking Vision Transformers via Heterogeneous Quantization Search}, 
  year={2025},
  preview={SpikeHQ.png},
  selected=True,
  }

@INPROCEEDINGS{Bishop,
  author = {Xu, Boxun and Yin, Yuxuan and Iyer, Vikram and Li, Peng},
  booktitle={International Symposium on Computer Architecture (ISCA)}, 
  title={Bishop: Sparsified Bundling Spiking Transformers on Heterogeneous Cores with Error-Constrained Pruning}, 
  year={2025},
  preview={Bishop.png},
  selected=True,
  }

@INPROCEEDINGS{SpikeX,
  author = {Xu, Boxun and Boone, Richard and Li, Peng},
  booktitle={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems(TCAD)}, 
  title={SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks}, 
  year={2025},
  preview={SpikeX.png},
  selected=True,
  }

@INPROCEEDINGS{JSSC-2024,
author = {Fan, Zichen and An, Hyochan and Zhang, Qirui and Xu, Boxun and Xu, Li and Tseng, Chien-Wei and Peng, Yimai and Cao, Ang and Liu, Bowen and Lee, Changwoo and Wang, Zhehong and Kim, Hun-Seok and Blaauw, David and Sylvester, Dennis},
booktitle={IEEE Journal of Solid-State Circuits(JSSC)}, 
title={AIMMI: Audio and Image Multi-Modal Intelligence via a Low-Power SoC With 2-MByte On-Chip MRAM for IoT Devices}, 
year={2024},
preview={JSSC24.png},
selected=True
}

@INPROCEEDINGS{3D-Spiking,
  author = {Xu, Boxun and Hwang, Junyoung and Vanna-iampikul,Pruek and Lim, Sung-Kyu and Li, Peng},
  booktitle={ACM/IEEE International Conference on Computer-Aided Design (ICCAD)}, 
  title={Spiking Transformer Hardware Accelerators in 3D Integration}, 
  year={2024},
  preview={3D-Spiking.png},
  selected=True,
  award = {Nomineed as William J. McCalla Best Paper Award}
  }

@INPROCEEDINGS{DS2TA,
author = {Xu, Boxun and Geng, Hejia and Yin, Yuxuan and Li, Peng},
booktitle={TMLR under review}, 
title={DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention}, 
year={2024},
preview={DS2TA.png},
selected=True
}

@INPROCEEDINGS{ADO-LLM,
  author = {Yin, Yuxuan and Wang, Yu and Xu, Boxun and Li, Peng},
  booktitle={ACM/IEEE International Conference on Computer-Aided Design (ICCAD)}, 
  title={ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of Large Language Models}, 
  year={2024},
  preview={ADO-LLM.png},
  selected=True
  }

@INPROCEEDINGS{VLSI-2022,
  author = {Fan, Zichen and An, Hyochan and Zhang, Qirui and Xu, Boxun and Xu, Li and Tseng, Chien-Wei and Peng, Yimai and Cao, Ang and Liu, Bowen and Lee, Changwoo and Wang, Zhehong and Liu, Fanghao and Wang, Guanru and Jiang, Shenghao and Kim, Hun-Seok and Blaauw, David and Sylvester, Dennis},
  booktitle={2022 IEEE Symposium on VLSI Technology and Circuits (VLSI-Symposium)}, 
  title={Audio and Image Cross-Modal Intelligence via a 10TOPS/W 22nm SoC with Back-Propagation and Dynamic Power Gating}, 
  year={2022},
  preview={VLSI22.png},
  selected=True
  }